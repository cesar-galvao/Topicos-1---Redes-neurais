x2.obs=runif(m.obs, -3, 3)) %>%
mutate(mu=abs(x1.obs^3 - 30*sin(x2.obs) + 10),
y=rnorm(m.obs, mean=mu, sd=1))
#| label: loss-function
#| depends: setup
J.Loss <- function(dados, theta, target){
if(!is.data.frame(dados) | !tibble::is_tibble(dados)){
stop("Loss: Dados deve ser uma dataframe ou tibble.")
} else if (dim(dados)[2] != 2){
stop("Loss: Matriz de dados deve ter 2 colunas.")
} else if (!is.list(theta)){
stop("Loss: Theta deve ser uma lista com pesos e viéses.")
} else if (!is.numeric(target)){
stop("Loss: Target deve ser um vetor numérico.")
}
# transpoe os dados para termos acesso aos vetores de x que serao passados para a primeira camada da rede
#tdados <- t(as.matrix(dados))
yhat <- ffwd(dados, theta)$yhat
a <- t(ffwd(dados, theta)$pre_activation)
return(
list(
loss = mean((target - yhat)^2), #média ja entrega soma/m
yhat = yhat,
pre_activation = a,
hidden = matrix(c(phi(a[,1]), phi(a[,2])), ncol = 2)
)
)
}
#| label: separacao-dados
#| depends: geracao-dados
dados_train <- dados[1:80000,]
dados_valid <- dados[80001:90000,]
dados_test <- dados[90001:nrow(dados),]
#| label: custo-rede
#| depends: feed-forward, loss-function, separacao-dados
#| output: false
# transformamos os dados em matriz
x_test <- dados_test %>%
select(x1.obs, x2.obs)
# separamos o target
y_test <- dados_test %>%
pull(y)
# theta já foi gerado e será reaproveitado
J.Loss(x_test, theta, y_test)$loss
#| label: gradiente
#| depends: setup
gradiente <- function(dados, theta, target){
if(!is.list(theta)){
stop("Gradiente: Theta deve ser uma lista com pesos e viéses.")
} else if(!is.data.frame(dados) | !tibble::is_tibble(dados)){
stop("Gradiente: Dados deve ser uma dataframe ou tibble.")
} else if(!is.numeric(target)){
stop("Gradiente: Target deve ser um vetor numérico.")
}
loss_results <- J.Loss(dados, theta, target)
# vetor de yhat
yhat <- loss_results$yhat
# pre_activation da J.Loss retorna o vetor de 'a'
a <- loss_results$pre_activation
#transformacao nos neuronios pre-ativacao
a <- exp(a)/(1+ exp(a))^2
# diferenca entre (target) y e yhat
diff <- (target-yhat)
# dados como uma matriz
dados <- as.matrix(dados)
g_w1 <- 2*mean((-1)*diff*theta[[3]][1]*dados[,1]*a[,1]) #x1, a1, w5
g_w2 <- 2*mean((-1)*diff*theta[[3]][2]*dados[,1]*a[,2]) #x1, a2, w6
g_w3 <- 2*mean((-1)*diff*theta[[3]][1]*dados[,2]*a[,1]) #x2, a1, w5
g_w4 <- 2*mean((-1)*diff*theta[[3]][2]*dados[,2]*a[,2]) #x2, a2, w6
g_w5 <- 2*mean(diff*(-loss_results$hidden[,1])) #h1
g_w6 <- 2*mean(diff*(-loss_results$hidden[,2])) #h2
g_b1 <- 2*mean((-1)*diff*theta[[3]][1]*a[,1]) #a1, w5
g_b2 <- 2*mean((-1)*diff*theta[[3]][2]*a[,2]) #a2, w6
g_b3 <- 2*mean(diff*(-1)) #b3
grad <- c(g_w1, g_w2, g_w3, g_w4, g_w5, g_w6, g_b1, g_b2, g_b3)
names(grad) <- c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3")
return(
list(grad = grad,
loss_results = list(
loss = loss_results$loss,
yhat = loss_results$yhat,
hidden = loss_results$hidden,
pre_activation = loss_results$pre_activation,
diff = diff
)
)
)
}
#| label: gradiente-aplicacao
#| depends: gradiente, separacao-dados
grad_item_d <- gradiente(dados_train[,c(1,2)], theta, dados_train$y)$grad
#| label: tbl-gradienteaplicacao
#| tbl-cap: Valores do gradiente para $\boldsymbol{\theta} = (0.1, \dots , 0.1)$
#| echo: false
grad_item_d %>%
as_tibble() %>%
reframe(Componente = c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3"),
Derivada = value) %>%
knitr::kable()
#| label: back-propagation
#| depends: setup
backpropagation <- function(dados, theta, target, learning_rate = NULL, epochs = NULL){
if(!is.list(theta)){
stop("Back propagation: Theta deve ser uma lista com pesos e viéses.")
} else if(!is.data.frame(dados) | !tibble::is_tibble(dados)){
stop("Back propagation: Dados deve ser uma dataframe ou tibble.")
} else if(!is.numeric(target)){
stop("Back propagation: Target deve ser um vetor numérico.")
} else if (!is.numeric(learning_rate) & !is.null(learning_rate) & learning_rate > 0){
stop("Back propagation: Learning rate deve ser um número real positivo.")
} else if (!is.integer(epochs) & !is.null(epochs)){
stop("Back propagation: Epochs deve ser um número inteiro positivo.")
}
# initiate history and best theta
loss_history <- numeric(epochs)
best_theta <- theta
# initiate loop
for(i in 1:epochs){
grad_results <- gradiente(dados, theta, target)
# fill in history
loss_history[i] <- grad_results$loss_results$loss
# fill in gradient list for update
gradient_list <- list(
W1_update = matrix(grad_results$grad[1:4], nrow = 2, byrow = TRUE),
b12_update = grad_results$grad[7:8],
W2_update = matrix(grad_results$grad[5:6], nrow = 2, byrow = TRUE),
b3_update = grad_results$grad[9]
)
# best theta condition
if (i >= 2){
if(loss_history[i] < loss_history[i-1]){
best_theta <- theta
}
}
# update theta with learning rate
theta <- list( #usaremos o mesmo nome para atualizar no loop
W1 = theta[[1]] - learning_rate*gradient_list$W1_update,
b12 = theta[[2]] - learning_rate*gradient_list$b12_update,
W2 = theta[[3]] - learning_rate*gradient_list$W2_update,
b3 = theta[[4]] - learning_rate*gradient_list$b3_update
)
}
# outputs: pesos finais e histórico de loss
return(list(
theta = theta,
loss_history = loss_history,
best_theta = best_theta
)
)
}
#| label: back-propagation-aplicacao
#| depends: back-propagation, separacao-dados
theta <- list(
W1 = matrix(rep(0, 4), nrow = 2, byrow = TRUE),
b12 = c(0, 0),
W2 = matrix(rep(0, 2), nrow = 2, byrow = TRUE),
b3 = 0
)
learning_rate <-  0.1
epochs <- 100L
y <- dados_valid$y
x <- dados_valid[,c(1,2)]
back_validacao <- backpropagation(x, theta, y, learning_rate, epochs)
#| label: fig-historico-custo
#| fig-cap: Histórico de custo para o banco de validação.
#| echo: false
#| fig-width: 4.5
#| fig-height: 2.75
back_validacao$loss_history %>%
tibble(Iteracao = 1:length(.), Custo = .) %>%
filter(Custo < 170) %>%
ggplot(aes(x = Iteracao, y = Custo)) +
geom_line() +
labs(x = "Iteração",
y = "Custo")+
theme_bw()+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm")) # Control tick length
#| label: fig-historico-custo-treinamento-validacao
#| fig-cap: Histórico de custo para o banco de treinamento e validação.
#| depends: back-propagation-aplicacao
#| fig-width: 4.5
#| fig-height: 3.2
# hiperparametros e base de treinamento
learning_rate <-  0.1
epochs <- 100L
y <- dados_train$y
x <- dados_train[,c(1,2)]
comparacao_treino_validacao <- tibble(
epoch = rep(1:epochs, 2),
base = rep(c("Validação", "Treinamento"), each = epochs),
# back_validacao foi gerada no item e
loss = c(back_validacao$loss_history, backpropagation(x, theta, y, learning_rate, epochs)$loss_history)
)
comparacao_treino_validacao %>%
filter(loss < 170) %>%
ggplot(aes(epoch, loss, group = base, color = base))+
geom_line(alpha = .5)+
labs(x = "Iteração",
y = "Custo",
color = "")+
theme_bw()+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom")
#| label: predicao-residuos
#| depends: gradiente
# usamos o melhor theta encontrado
theta <- back_validacao$best_theta
resultados_gradiente <- gradiente(dados_test[,c(1,2)], theta, dados_test$y)
residuos = resultados_gradiente$loss_results$diff
# usamos o conjunto de teste
dados_graf_residuos <- dados_test %>%
select(x1.obs, x2.obs, y) %>% #seleciona variaveis de interesse
mutate(residuos = residuos,
yhat = resultados_gradiente$loss_results$yhat
)
# base do gráfico
plot_residuos <- ggplot(dados_graf_residuos, aes(x=x1.obs, y=x2.obs)) +
geom_point(aes(colour=residuos), size=2, shape=15, alpha = .3) +
coord_cartesian(expand=F) +
scale_colour_gradient(low="white",
high="black",
name="Resíduos") +
xlab(TeX("$X_1$")) + ylab(TeX("$X_2$"))+
theme(legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5))
#gráfico da esperança
n <- 100
x1 <- seq(-3, 3, length.out=n)
x2 <- seq(-3, 3, length.out=n)
dados.grid <- as_tibble(expand.grid(x1, x2)) %>%
rename_all(~ c("x1", "x2")) %>%
mutate(mu=abs(x1^3 - 30*sin(x2) + 10))
plot_esperanca <- ggplot(dados.grid, aes(x=x1, y=x2)) +
geom_point(aes(colour=mu), size=2, shape=15) +
coord_cartesian(expand=F) +
scale_colour_gradient(low="white",
high="black",
name=TeX("$E(Y|X_1, X_2)$")) +
xlab(TeX("$X_1$")) + ylab(TeX("$X_2$"))+
theme(legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5))
#| label: fig-predicao-residuos
#| fig-cap: Resíduos da rede em função de $X_1$ e $X_2$.
#| depends: predicao-residuos
plot_grid(plot_residuos, plot_esperanca, ncol=2)
#| label: fig-esperadoobservado
#| fig-cap: Valor observado em função do valor esperado gerado pela rede neural.
dados_graf_residuos %>%
ggplot(aes(yhat, y))+
geom_point(alpha = 0.3)+
geom_abline(slope = 1, intercept = 0, color = "red")+
theme_bw()+
labs(x = TeX("$\\hat{y}_i$"), y = TeX("$y_i$"))+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5))
#| label: gradiente-k
theta <- list(
M1 = matrix(c(0.1), nrow = 2, ncol = 2),
b12 = c(0.1, 0.1),
M2 = matrix(c(0.1), nrow = 2, ncol = 1),
b3 = c(0.1)
)
k_w1 <- function(dados, theta, i){
base <- dados[1:i,c(1, 2)] #dados com linha 1 ate i
y <- dados$y[1:i] #target com linhas 1 até i
return(gradiente(base, theta, y)$grad[1]) #retorna w1
}
valores_w1 <- tibble(
k = 1:300,
w1 = map_dbl(k, ~ k_w1(dados, theta, .))
)
#| label: fig-gradientek
#| fig-cap: Valor do gradiente $\frac{\partial J}{\partial w_1}$ em função do tamanho da amostra.
ggplot(valores_w1, aes(k, w1, group = 1))+
geom_line()+
geom_hline(yintercept = grad_item_d["w1"], color = "red")+
theme_bw()+
labs(x = "Tamanho da amostra", y = TeX("$w_1$"))+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5))+
annotate("text", x = 200, y = 0,
label = paste0("w1 = ",round(grad_item_d["w1"],2)),
hjust = 1, vjust = 0, size = 4, color = "red")
#| label: tbl-microbenchmark
#| tbl-cap: Resultados do microbenchmark para o cálculo do gradiente com k = 300 e k = 100.000.
microbenchmark::microbenchmark(
k_300 = k_w1(dados, theta, 300),
k_100000 = k_w1(dados, theta, 100000),
times = 100
) %>%
summary() %>%
select(-neval) %>%
knitr::kable(digits = 2)
#| label: ajuste-linear
MSE1 <- lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova() %>%
pull(`Mean Sq`) %>%
min()
MSE2 <- lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train) %>%
anova() %>%
pull(`Mean Sq`) %>%
min()
MSErn <- min(back_validacao$loss_history)
#| label: tbl-MSE
#| echo: false
#| tbl-cap: Perdas quadráticas dos modelos ajustados.
tibble(
Modelo = c("Rede Neural", "Modelo Linear 1", "Modelo Linear 2"),
MSE = c(MSErn, MSE1, MSE2)
) %>%
mutate(MSE = round(MSE, 2)) %>%
knitr::kable()
#| label: efeito-x1
#| echo: false
x1m1 <- lm(y ~ x1.obs + x2.obs, data = dados_train)
x1m2 <-  lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train)
x2rn <- back_validacao$best_theta$W1
#| label: tabela-efeito-x1
efeitos_x1 <- tibble(
x1 = seq(-3, 3, length.out = 100),
x20 = rep(0, 100),
x215 = rep(1.5, 100),
x2_15 = rep(-1.5, 100),
x23 = rep(3, 100)
) %>%
mutate(yhat0 = ffwd(.[,c(1,2)], back_validacao$best_theta)$yhat,
yhat15 = ffwd(.[,c(1,3)], back_validacao$best_theta)$yhat,
yhat15_ = ffwd(.[,c(1,4)], back_validacao$best_theta)$yhat,
yhat3 = ffwd(.[,c(1,5)], back_validacao$best_theta)$yhat) %>%
select(x1, yhat0, yhat15, yhat15_, yhat3) %>%
pivot_longer(cols = -x1, names_to = "x2", values_to = "yhat") %>%
mutate(x2 = case_when(
x2 == "yhat0" ~ '0',
x2 == "yhat15" ~ '1.5',
x2 == "yhat15_" ~ '-1.5',
x2 == "yhat3" ~ '3'
)
)
#| label: fig-efeitox1
#| fig-cap: Efeito de um incremento unitário em $x_1$ no valor esperado da variável resposta.
#| echo: false
ggplot(efeitos_x1, aes(x = x1, y = yhat, color = x2, group = x2))+
geom_line()+
scale_x_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3))+
theme_bw()+
labs(x = TeX("$x_1$"), y = TeX("$\\hat{y}$"), color = TeX("Valor de $x_2$"))+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "right",
axis.title.y = element_text(angle = 0, vjust = 0.5))
#| label: intervalos-confianca
# modelo 1
predictm1 <- predict(x1m1, interval="prediction", newdata = dados_test) %>%
tidy() %>%
as.matrix(ncol = 3)
withinm1 <- (dados_test$y >= predictm1[,2] & dados_test$y <= predictm1[,3]) %>%
mean()
# modelo 2
predictm2 <- predict(x1m2, interval="prediction", data = dados_test) %>%
tidy() %>%
as.matrix(ncol = 3)
withinm2 <- (dados_test$y >= predictm2[,2] & dados_test$y <= predictm2[,3]) %>%
mean()
# rede neural
tabela_withinrn <- tibble(
yhat = ffwd(dados_test[,c(1,2)], back_validacao$best_theta)$yhat, #usamos o melhor theta da validação
sigma = sqrt(min(back_validacao$loss_history)), #usamos o minimo da loss da validação
y = dados_test$y,
std_y = (y-yhat)/sigma
) %>%
mutate(
lower = yhat - qnorm(0.975)*sigma,
upper = yhat + qnorm(0.025)*sigma,
within = (y >= lower & y <= upper)
)
withinrn <- tabela_withinrn%>%
pull(within) %>%
mean()
tabela_withinrn
tabela_withinrn%>%
pull(within) %>%
mean()
# rede neural
tabela_withinrn <- tibble(
yhat = ffwd(dados_test[,c(1,2)], back_validacao$best_theta)$yhat, #usamos o melhor theta da validação
sigma = sqrt(min(back_validacao$loss_history)), #usamos o minimo da loss da validação
y = dados_test$y,
std_y = (y-yhat)/sigma
) %>%
mutate(
lower = yhat - qnorm(0.975)*sigma,
upper = yhat + qnorm(0.975)*sigma,
within = (y >= lower & y <= upper)
)
withinrn <- tabela_withinrn%>%
pull(within) %>%
mean()
tabela_withinrn
tabela_withinrn%>%
pull(within) %>%
mean()
#| label: tbl-intervalos-confianca
#| echo: false
#| tbl-cap: Proporção de valores de $y_i$ capturados pelos intervalos de confiança de 95%.
tibble(
Modelo = c("Modelo Linear 1", "Modelo Linear 2", "Rede Neural"),
Proporção = c(withinm1, withinm2, withinrn),
MSE = c(MSE1, MSE2,MSErn)
) %>%
knitr::kable(digits = 2)
#| label: fig-intervaloconfianca
#| fig-cap: Pontos contidos nos intervalos de confiança de 95% para o Modelo Linear 1.
tabela <- as_tibble(predictm1) %>%
rlang::set_names("yhat","lower", "upper") %>%
mutate(y = dados_test$y,
within = (y >= lower & y <= upper),
x1 = dados_test$x1.obs,
x2 = dados_test$x2.obs)
plot_regressao <- ggplot(tabela, aes(x1, x2, color = within))+
geom_point(alpha = 0.09)+
scale_color_manual(values = c("green", "red")) +
theme_bw()+
labs(x = TeX("$x_1$"), y = TeX("$x_2$"), color = "Dentro do IC")+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5))
plot_grid(plot_regressao, plot_esperanca, ncol=2)
#| label: fig-intervaloconfiancarede
#| fig-cap: Pontos contidos nos intervalos de confiança de 95% para a Rede Neural.
tabela_withinrn %>%
mutate(x1 = dados_test$x1.obs,
x2 = dados_test$x2.obs) %>%
ggplot(aes(x1, x2, color = within))+
scale_color_manual(values = c("green", "red")) +
geom_point(alpha = 0.12)+
theme_bw()+
labs(x = TeX("$x_1$"), y = TeX("$x_2$"), color = "Dentro do IC")+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5),
aspect.ratio = 1)
#| label: setup
#| echo: false
pacman::p_load(tidyverse, NeuralNetTools,latex2exp, cowplot, microbenchmark, tidymodels)
#| label: fig-intervaloconfianca
#| fig-cap: Pontos contidos nos intervalos de confiança de 95% para o Modelo Linear 1.
tabela <- as_tibble(predictm1) %>%
rlang::set_names("yhat","lower", "upper") %>%
mutate(y = dados_test$y,
within = (y >= lower & y <= upper),
x1 = dados_test$x1.obs,
x2 = dados_test$x2.obs)
plot_regressao <- ggplot(tabela, aes(x1, x2, color = within))+
geom_point(alpha = 0.09)+
scale_color_manual(values = c("red","green")) +
theme_bw()+
labs(x = TeX("$x_1$"), y = TeX("$x_2$"), color = "Dentro do IC")+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5))
plot_grid(plot_regressao, plot_esperanca, ncol=2)
#| label: fig-intervaloconfiancarede
#| fig-cap: Pontos contidos nos intervalos de confiança de 95% para a Rede Neural.
tabela_withinrn %>%
mutate(x1 = dados_test$x1.obs,
x2 = dados_test$x2.obs) %>%
ggplot(aes(x1, x2, color = within))+
scale_color_manual(values = c("red","green")) +
geom_point(alpha = 0.12)+
theme_bw()+
labs(x = TeX("$x_1$"), y = TeX("$x_2$"), color = "Dentro do IC")+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5),
aspect.ratio = 1)
