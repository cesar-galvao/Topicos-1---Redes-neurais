300/100000
517/36438
?microbenchmark
res <- microbenchmark::microbenchmark(
k_300 = k_w1(dados, theta, 300),
k_100000 = k_w1(dados, theta, 100000),
times = 100
)
res$expr
res$time
boxplot(res)
microbenchmark::microbenchmark(
k_300 = k_w1(dados, theta, 300),
k_100000 = k_w1(dados, theta, 100000),
times = 100
)
524/38025
valores_w1 <- tibble(
k = 1:1000,
w1 = map_dbl(k, ~ k_w1(dados, theta, .))
)
mean(valores_w1$w1)
res
tidy(res)
library(tidymodels)
install.packages("tidymodels")
library(tidymodels)
tidy(res)
res
View(res)
summary(res)
summary(res) %>% class()
325/22178
2058/251353
552/41206
300/100000
lm(y ~ x1.obs + x2.obs, data = dados_train)
fit <- lm(y ~ x1.obs + x2.obs, data = dados_train)
summary(fit)
anova(fit)
t2 <- anova(fit)
t2$`Mean Sq`
anova(fit)['Residuals', 'Mean Sq']
11096483/79997
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova()
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova() %>% class()
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova() %>%
pull(`Mean Sq`)
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova() %>%
pull(`Mean Sq`)[3]
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova() %>%
pull(`Mean Sq`)
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova() %>%
pull(`Mean Sq`) %>%
min()
lm(y ~ x1.obs:x2.obs , data = dados_train)
lm(y ~ x1.obs*x2.obs , data = dados_train)
lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train)
lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train) %>%
anova()
lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train) %>%
anova() %>%
pull(`Mean Sq`)
lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train) %>%
anova() %>%
pull(`Mean Sq`) %>%
min()
MSE1 <- lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova() %>%
pull(`Mean Sq`) %>%
min()
MSE2 <- lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train) %>%
anova() %>%
pull(`Mean Sq`) %>%
min()
min(back_validacao$loss_history)
tibble(
Modelo = c("Rede Neural", "Modelo Linear 1", "Modelo Linear 2"),
MSE = c(min(back_validacao$loss_history), MSE1, MSE2)
) %>%
knitr::kable()
tibble(
Modelo = c("Rede Neural", "Modelo Linear 1", "Modelo Linear 2"),
MSE = c(min(back_validacao$loss_history), MSE1, MSE2)
)
tibble(
Modelo = c("Rede Neural", "Modelo Linear 1", "Modelo Linear 2"),
MSE = c(min(back_validacao$loss_history), MSE1, MSE2)
) %>%
mutate(MSE = round(MSE, 2)) %>%
knitr::kable()
tibble(
Modelo = c("Rede Neural", "Modelo Linear 1", "Modelo Linear 2"),
MSE = c(min(back_validacao$loss_history), MSE1, MSE2)
) %>%
mutate(MSE = round(MSE, 2))
#| label: setup
#| echo: false
pacman::p_load(tidyverse, NeuralNetTools,latex2exp, cowplot, microbenchmark)
#| label: figf-arquitetura-rede
#| fig-cap: "Arquitetura da rede neural artificial. Adotamos função de ativação sigmoide e linear nas camadas escondidas e de saída, respectivamente."
#| fig-width: 4
#| echo: false
### Figura 2: Arquitetura da RNA.
par(mar=c(0, 0, 0, 0))
wts_in <- rep(1, 9)
struct <- c(2, 2, 1) # dois inputs, dois neurônios escondidos e um otput
plotnet(wts_in, struct = struct,
x_names="", y_names="",
node_labs=F, rel_rsc=.7)
aux <- list(
x=c(-.8, -.8, 0, 0, .8, rep(-.55, 4), -.12, -0.06, .38, .38, .7),
y=c(.73, .28, .73, .28, .5, .78, .68, .48, .32, .88, .5, .68, .44, .7),
rotulo=c("x_1", "x_2", "h_1", "h_2", "\\hat{y}", paste0("w_", 1:4),
"b_1", "b_2", "w_5", "w_6", "b_3")
)
walk(transpose(aux), ~ text(.$x, .$y,
TeX(str_c("$", .$rotulo, "$")), cex=.8))
#| label: feed-forward
# Função de ativação
phi <- function(x) {1/(1 + exp(-x))}
# Função de previsão
ffwd <- function(x, theta) {
if(any(dim(theta[[1]]) != c(2, 2))){
stop("Feed Forward: O primeiro elemento de theta deve ser uma matriz de pesos 2x2 para a aplicação nos dados")
} else if(length(theta[[2]]) != 2){
stop("Feed Forward: O segundo elemento de theta deve ser um vetor viés de tamanho 2 para somar aos dados")
} else if(any(dim(theta[[3]]) != c(2, 1))){
stop("Feed Forward: O terceiro elemento de theta deve ser uma matriz de pesos 2x1 para aplicação na única camada h")
} else if(length(theta[[4]]) != 1){
stop("Feed Forward: O quarto elemento de theta deve ser um vetor viés de tamanho 1 para soma na única camada h")
} else if(!is.data.frame(x) | !tibble::is_tibble(x) & dim(x)[2] != 2){
stop("Feed Forward: x deve ser uma dataframe ou tibble com 2 colunas")
}
x <- as.matrix(x)
W1 <- theta[[1]]
b1 <- theta[[2]]
W2 <- theta[[3]]
b2 <- theta[[4]]
a <- (t(W1)%*%t(x))+b1
h <- phi(a)
yhat <- (t(W2)%*%h)+b2
return( # separacao dos elementos de saída para usar no backpropagation
list(yhat = as.double(yhat),
hidden = h,
pre_activation = a)
)
}
#| label: feed-forward-aplicacao
#| depends: feed-forward
#| output: false
x <- data.frame(x1 = 1, x2 = 1)
theta <- list(
M1 = matrix(c(0.1), nrow = 2, ncol = 2),
b12 = c(0.1, 0.1),
M2 = matrix(c(0.1), nrow = 2, ncol = 1),
b3 = c(0.1)
)
ffwd(x, theta)
#| include: false
rm(x)
#| label: geracao-dados
### Gerando dados "observados"
set.seed(1.2024)
m.obs <- 100000
dados <- tibble(x1.obs=runif(m.obs, -3, 3),
x2.obs=runif(m.obs, -3, 3)) %>%
mutate(mu=abs(x1.obs^3 - 30*sin(x2.obs) + 10),
y=rnorm(m.obs, mean=mu, sd=1))
#| label: loss-function
#| depends: setup
# VERIFICAR A CONTA AQUI NO FINAL E PASSAR PARA MATRICIAL
J.Loss <- function(dados, theta, target){
if(!is.data.frame(dados) | !tibble::is_tibble(dados)){
stop("Loss: Dados deve ser uma dataframe ou tibble.")
} else if (dim(dados)[2] != 2){
stop("Loss: Matriz de dados deve ter 2 colunas.")
} else if (!is.list(theta)){
stop("Loss: Theta deve ser uma lista com pesos e viéses.")
} else if (!is.numeric(target)){
stop("Loss: Target deve ser um vetor numérico.")
}
# transpoe os dados para termos acesso aos vetores de x que serao passados para a primeira camada da rede
#tdados <- t(as.matrix(dados))
yhat <- ffwd(dados, theta)$yhat
a <- t(ffwd(dados, theta)$pre_activation)
return(
list(
loss = mean((target - yhat)^2), #média ja entrega soma/m
yhat = yhat,
pre_activation = a,
hidden = matrix(c(phi(a[,1]), phi(a[,2])), ncol = 2)
)
)
}
#| label: separacao-dados
#| depends: geracao-dados
dados_train <- dados[1:80000,]
dados_valid <- dados[80001:90000,]
dados_test <- dados[90001:nrow(dados),]
#| label: custo-rede
#| depends: feed-forward, loss-function, separacao-dados
#| output: false
# transformamos os dados em matriz
x_test <- dados_test %>%
select(x1.obs, x2.obs)
# separamos o target
y_test <- dados_test %>%
pull(y)
# theta já foi gerado e será reaproveitado
J.Loss(x_test, theta, y_test)$loss
#| label: gradiente
#| depends: setup
gradiente <- function(dados, theta, target){
if(!is.list(theta)){
stop("Gradiente: Theta deve ser uma lista com pesos e viéses.")
} else if(!is.data.frame(dados) | !tibble::is_tibble(dados)){
stop("Gradiente: Dados deve ser uma dataframe ou tibble.")
} else if(!is.numeric(target)){
stop("Gradiente: Target deve ser um vetor numérico.")
}
loss_results <- J.Loss(dados, theta, target)
# vetor de yhat
yhat <- loss_results$yhat
# pre_activation da J.Loss retorna o vetor de 'a'
a <- loss_results$pre_activation
#transformacao nos neuronios pre-ativacao
a <- exp(a)/(1+ exp(a))^2
# diferenca entre (target) y e yhat
diff <- (target-yhat)
# dados como uma matriz
dados <- as.matrix(dados)
g_w1 <- 2*mean((-1)*diff*theta[[3]][1]*dados[,1]*a[,1]) #x1, a1, w5
g_w2 <- 2*mean((-1)*diff*theta[[3]][2]*dados[,1]*a[,2]) #x1, a2, w6
g_w3 <- 2*mean((-1)*diff*theta[[3]][1]*dados[,2]*a[,1]) #x2, a1, w5
g_w4 <- 2*mean((-1)*diff*theta[[3]][2]*dados[,2]*a[,2]) #x2, a2, w6
g_w5 <- 2*mean(diff*(-loss_results$hidden[,1])) #h1
g_w6 <- 2*mean(diff*(-loss_results$hidden[,2])) #h2
g_b1 <- 2*mean((-1)*diff*theta[[3]][1]*a[,1]) #a1, w5
g_b2 <- 2*mean((-1)*diff*theta[[3]][2]*a[,2]) #a2, w6
g_b3 <- 2*mean(diff*(-1)) #b3
grad <- c(g_w1, g_w2, g_w3, g_w4, g_w5, g_w6, g_b1, g_b2, g_b3)
names(grad) <- c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3")
return(
list(grad = grad,
loss_results = list(
loss = loss_results$loss,
yhat = loss_results$yhat,
hidden = loss_results$hidden,
pre_activation = loss_results$pre_activation,
diff = diff
)
)
)
}
#| label: gradiente-aplicacao
#| depends: gradiente, separacao-dados
grad_item_d <- gradiente(dados_train[,c(1,2)], theta, dados_train$y)$grad
grad_item_d %>%
as_tibble() %>%
reframe(Componente = c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3"),
Derivada = value) %>%
knitr::kable()
#| label: back-propagation
#| depends: setup
backpropagation <- function(dados, theta, target, learning_rate = NULL, epochs = NULL){
if(!is.list(theta)){
stop("Back propagation: Theta deve ser uma lista com pesos e viéses.")
} else if(!is.data.frame(dados) | !tibble::is_tibble(dados)){
stop("Back propagation: Dados deve ser uma dataframe ou tibble.")
} else if(!is.numeric(target)){
stop("Back propagation: Target deve ser um vetor numérico.")
} else if (!is.numeric(learning_rate) & !is.null(learning_rate) & learning_rate > 0){
stop("Back propagation: Learning rate deve ser um número real positivo.")
} else if (!is.integer(epochs) & !is.null(epochs)){
stop("Back propagation: Epochs deve ser um número inteiro positivo.")
}
# initiate history and best theta
loss_history <- numeric(epochs)
best_theta <- theta
# initiate loop
for(i in 1:epochs){
grad_results <- gradiente(dados, theta, target)
# fill in history
loss_history[i] <- grad_results$loss_results$loss
# fill in gradient list for update
gradient_list <- list(
W1_update = matrix(grad_results$grad[1:4], nrow = 2, byrow = TRUE),
b12_update = grad_results$grad[7:8],
W2_update = matrix(grad_results$grad[5:6], nrow = 2, byrow = TRUE),
b3_update = grad_results$grad[9]
)
# best theta condition
if (i >= 2){
if(loss_history[i] < loss_history[i-1]){
best_theta <- theta
}
}
# update theta with learning rate
theta <- list( #usaremos o mesmo nome para atualizar no loop
W1 = theta[[1]] - learning_rate*gradient_list$W1_update,
b12 = theta[[2]] - learning_rate*gradient_list$b12_update,
W2 = theta[[3]] - learning_rate*gradient_list$W2_update,
b3 = theta[[4]] - learning_rate*gradient_list$b3_update
)
}
# outputs: pesos finais e histórico de loss
return(list(
theta = theta,
loss_history = loss_history,
best_theta = best_theta
)
)
}
#| label: back-propagation-aplicacao
#| depends: back-propagation, separacao-dados
theta <- list(
W1 = matrix(rep(0, 4), nrow = 2, byrow = TRUE),
b12 = c(0, 0),
W2 = matrix(rep(0, 2), nrow = 2, byrow = TRUE),
b3 = 0
)
learning_rate <-  0.1
epochs <- 100L
y <- dados_valid$y
x <- dados_valid[,c(1,2)]
back_validacao <- backpropagation(x, theta, y, learning_rate, epochs)
#| label: fig-historico-custo
#| fig-cap: Histórico de custo para o banco de validação.
#| echo: false
#| fig-width: 4.5
#| fig-height: 2.75
back_validacao$loss_history %>%
tibble(Iteracao = 1:length(.), Custo = .) %>%
ggplot(aes(x = Iteracao, y = Custo)) +
geom_line() +
labs(x = "Iteração",
y = "Custo")+
theme_bw()+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm")) # Control tick length
#| label: fig-historico-custo-treinamento-validacao
#| fig-cap: Histórico de custo para o banco de treinamento e validação.
#| depends: back-propagation-aplicacao
#| fig-width: 4.5
#| fig-height: 3.2
# hiperparametros e base de treinamento
learning_rate <-  0.1
epochs <- 100L
y <- dados_train$y
x <- dados_train[,c(1,2)]
comparacao_treino_validacao <- tibble(
epoch = rep(1:epochs, 2),
base = rep(c("Validação", "Treinamento"), each = epochs),
# back_validacao foi gerada no item e
loss = c(back_validacao$loss_history, backpropagation(x, theta, y, learning_rate, epochs)$loss_history)
)
comparacao_treino_validacao %>%
filter(loss < 170) %>%
ggplot(aes(epoch, loss, group = base, color = base))+
geom_line(alpha = .5)+
labs(x = "Iteração",
y = "Custo",
color = "")+
theme_bw()+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom")
#| label: predicao-residuos
#| depends: gradiente
#calcular o back propagation e usar o melhor theta do conjunto de teste
# usamos o conjunto de teste
theta <- back_validacao$theta
resultados_gradiente <- gradiente(dados_test[,c(1,2)], theta, dados_test$y)
residuos = resultados_gradiente$loss_results$diff
# usamos o conjunto de teste
dados_graf_residuos <- dados_test %>%
select(x1.obs, x2.obs, y) %>% #seleciona variaveis de interesse
mutate(residuos = residuos,
yhat = resultados_gradiente$loss_results$yhat
)
# base do gráfico
plot_residuos <- ggplot(dados_graf_residuos, aes(x=x1.obs, y=x2.obs)) +
geom_point(aes(colour=residuos), size=2, shape=15, alpha = .3) +
coord_cartesian(expand=F) +
scale_colour_gradient(low="white",
high="black",
name="Resíduos") +
xlab(TeX("$X_1$")) + ylab(TeX("$X_2$"))+
theme(legend.position = "bottom")
#gráfico da esperança
n <- 100
x1 <- seq(-3, 3, length.out=n)
x2 <- seq(-3, 3, length.out=n)
dados.grid <- as_tibble(expand.grid(x1, x2)) %>%
rename_all(~ c("x1", "x2")) %>%
mutate(mu=abs(x1^3 - 30*sin(x2) + 10))
plot_esperanca <- ggplot(dados.grid, aes(x=x1, y=x2)) +
geom_point(aes(colour=mu), size=2, shape=15) +
coord_cartesian(expand=F) +
scale_colour_gradient(low="white",
high="black",
name=TeX("$E(Y|X_1, X_2)$")) +
xlab(TeX("$X_1$")) + ylab(TeX("$X_2$"))+
theme(legend.position = "bottom")
#| label: fig-predicao-residuos
#| fig-cap: Resíduos da rede em função de $X_1$ e $X_2$.
#| depends: predicao-residuos
plot_grid(plot_residuos, plot_esperanca, ncol=2)
#| label: fig-esperado-observado
dados_graf_residuos %>%
ggplot(aes(yhat, y))+
geom_point(alpha = 0.3)+
geom_abline(slope = 1, intercept = 0, color = "red")+
theme_bw()+
labs(x = TeX("$\\hat{y}_i$"), y = TeX("$y_i$"))+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5))
#| label: gradiente-k
theta <- list(
M1 = matrix(c(0.1), nrow = 2, ncol = 2),
b12 = c(0.1, 0.1),
M2 = matrix(c(0.1), nrow = 2, ncol = 1),
b3 = c(0.1)
)
k_w1 <- function(dados, theta, i){
base <- dados[1:i,c(1, 2)] #dados com linha 1 ate i
y <- dados$y[1:i] #target com linhas 1 até i
return(gradiente(base, theta, y)$grad[1]) #retorna w1
}
valores_w1 <- tibble(
k = 1:300,
w1 = map_dbl(k, ~ k_w1(dados, theta, .))
)
#| label: fig-gradientek
ggplot(valores_w1, aes(k, w1, group = 1))+
geom_line()+
geom_hline(yintercept = grad_item_d["w1"], color = "red")+
theme_bw()+
labs(x = "Tamanho da amostra", y = TeX("$w_1$"))+
theme(panel.border = element_blank(), # Remove all panel borders
axis.line = element_line(color = "#474747"), # Add axis lines
axis.ticks.x = element_line(color = "#474747"), # X axis ticks
axis.ticks.y = element_line(color = "#474747"), # Y axis ticks
axis.ticks.length = unit(0.05, "cm"), # Control tick length
legend.position = "bottom",
axis.title.y = element_text(angle = 0, vjust = 0.5))+
annotate("text", x = 200, y = 0,
label = paste0("w1 = ",round(grad_item_d["w1"],2)),
hjust = 1, vjust = 0, size = 4, color = "red")
#| label: microbenchmark-gradiente
res <- microbenchmark::microbenchmark(
k_300 = k_w1(dados, theta, 300),
k_100000 = k_w1(dados, theta, 100000),
times = 100
)
#| label: ajuste-linear
dados_train
MSE1 <- lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova() %>%
pull(`Mean Sq`) %>%
min()
MSE2 <- lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train) %>%
anova() %>%
pull(`Mean Sq`) %>%
min()
#| label: tbl-MSE
tibble(
Modelo = c("Rede Neural", "Modelo Linear 1", "Modelo Linear 2"),
MSE = c(min(back_validacao$loss_history), MSE1, MSE2)
) %>%
mutate(MSE = round(MSE, 2)) %>%
knitr::kable()
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
anova()
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
summary()
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
summary() %>% class
lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
summary() %>% tidy
back_validacao$best_theta
x1m1 <- lm(y ~ x1.obs + x2.obs, data = dados_train) %>%
summary() %>%
tidy()
x1m2 <-  lm(y ~ x1.obs*x2.obs + I(x1.obs^2) + I(x2.obs^2), data = dados_train) %>%
summary() %>%
tidy()
x1m1
x1m1$estimate[2]
x1m2
x2rn
x2rn <- back_validacao$best_theta$W1
x2rn
x2rn[1,]
