a <- loss_results$pre_activation
#transformacao em nos neuronios pre-ativacao
a[,1] <- exp(a[,1])/(1+ exp(a[,1]))^2
a[,2] <- exp(a[,2])/(1+ exp(a[,2]))^2
# diferenca entre (target) y e yhat
diff <- (target-yhat)
# dados como uma matriz
dados <- as.matrix(dados)
g_w1 <- 2*mean(diff*theta[[3]][1]*dados[,1]*a[,1]) #x1, a1, w5
g_w2 <- 2*mean(diff*theta[[3]][2]*dados[,1]*a[,2]) #x1, a2, w6
g_w3 <- 2*mean(diff*theta[[3]][1]*dados[,2]*a[,1]) #x2, a1, w5
g_w4 <- 2*mean(diff*theta[[3]][2]*dados[,2]*a[,2]) #x2, a2, w6
g_w5 <- 2*mean(diff*(-loss_results$hidden[,1])) #h1
g_w6 <- 2*mean(diff*(-loss_results$hidden[,2])) #h2
g_b1 <- 2*mean(diff*theta[[3]][1]*a[,1]) #a1, w5
g_b2 <- 2*mean(diff*theta[[3]][2]*a[,2]) #a2, w6
g_b3 <- 2*mean(diff*(-1))
grad <- c(g_w1, g_w2, g_w3, g_w4, g_w5, g_w6, g_b1, g_b2, g_b3)
names(grad) <- c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3")
return(grad)
}
gradiente <- function(theta, dados, target){
if(!is.list(theta)){
stop("Theta deve ser uma lista com pesos e viéses.")
} else if(!is.data.frame(dados) | !is_tibble(dados)){
stop("Dados deve ser uma dataframe ou tibble.")
} else if(!is.numeric(target)){
stop("Target deve ser um vetor numérico.")
}
loss_results <- J.Loss(dados, theta, target)
# vetor de yhat
yhat <- loss_results$yhat
# pre_activation da J.Loss retorna o vetor de 'a'
a <- loss_results$pre_activation
#transformacao em nos neuronios pre-ativacao
a[,1] <- exp(a[,1])/(1+ exp(a[,1]))^2
a[,2] <- exp(a[,2])/(1+ exp(a[,2]))^2
# diferenca entre (target) y e yhat
diff <- (target-yhat)
# dados como uma matriz
dados <- as.matrix(dados)
g_w1 <- 2*mean(diff*theta[[3]][1]*dados[,1]*a[,1]) #x1, a1, w5
g_w2 <- 2*mean(diff*theta[[3]][2]*dados[,1]*a[,2]) #x1, a2, w6
g_w3 <- 2*mean(diff*theta[[3]][1]*dados[,2]*a[,1]) #x2, a1, w5
g_w4 <- 2*mean(diff*theta[[3]][2]*dados[,2]*a[,2]) #x2, a2, w6
g_w5 <- 2*mean(diff*(-loss_results$hidden[,1])) #h1
g_w6 <- 2*mean(diff*(-loss_results$hidden[,2])) #h2
g_b1 <- 2*mean(diff*theta[[3]][1]*a[,1]) #a1, w5
g_b2 <- 2*mean(diff*theta[[3]][2]*a[,2]) #a2, w6
g_b3 <- 2*mean(diff*(-1))
grad <- c(g_w1, g_w2, g_w3, g_w4, g_w5, g_w6, g_b1, g_b2, g_b3)
names(grad) <- c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3")
return(grad)
}
#| label: gradiente-aplicacao
#| depends: gradiente, separacao-dados
gradiente(theta, dados_train[,c(1,2)], dados_train$y)
x <- c(1,1)
theta <- list(
matrix(c(0.1), nrow = 2, ncol = 2), #w1
c(0.1, 0.1), #b1
matrix(c(0.1), nrow = 2, ncol = 1), #w2
c(0.1) #b2
)
x <- as.vector(x)
W1 <- theta[[1]]
b1 <- theta[[2]]
W2 <- theta[[3]]
b2 <- theta[[4]]
a <- (t(W1)%*%x)+b1
head(a)
phi(a)
h <- phi(a)
yhat <- (t(W2)%*%h)+b2
return( # separacao dos elementos de saída para usar no backpropagation
list(yhat = as.double(yhat),
hidden = h,
pre_activation = a)
)
# Função de previsão
ffwd <- function(x, theta) {
if(any(dim(theta[[1]]) != c(2, 2))){
stop("O primeiro elemento de theta deve ser uma matriz de pesos 2x2 para a aplicação nos dados")
} else if(length(theta[[2]]) != 2){
stop("O segundo elemento de theta deve ser um vetor viés de tamanho 2 para somar aos dados")
} else if(any(dim(theta[[3]]) != c(2, 1))){
stop("O terceiro elemento de theta deve ser uma matriz de pesos 2x1 para aplicação na única camada h")
} else if(length(theta[[4]]) != 1){
stop("O quarto elemento de theta deve ser um vetor viés de tamanho 1 para soma na única camada h")
} else if(!is.vector(x) & length(x) != 2){
stop("O x deve ser um vetor de tamanho 2")
}
x <- as.vector(x)
W1 <- theta[[1]]
b1 <- theta[[2]]
W2 <- theta[[3]]
b2 <- theta[[4]]
a <- (t(W1)%*%x)+b1
h <- phi(a)
yhat <- (t(W2)%*%h)+b2
return( # separacao dos elementos de saída para usar no backpropagation
list(yhat = as.double(yhat),
hidden = h,
pre_activation = a)
)
}
#| label: feed-forward-aplicacao
#| depends: feed-forward
#| output: false
x <- c(1,1)
theta <- list(
matrix(c(0.1), nrow = 2, ncol = 2), #w1
c(0.1, 0.1), #b1
matrix(c(0.1), nrow = 2, ncol = 1), #w2
c(0.1) #b2
)
ffwd(x, theta)
head(a)
J.Loss <- function(dados, theta, target){
if(!is.data.frame(dados) | !is_tibble(dados)){
stop("Dados deve ser uma dataframe ou tibble.")
} else if (dim(dados)[2] != 2){
stop("Matriz de dados deve ter 2 colunas.")
} else if (!is.list(theta)){
stop("Theta deve ser uma lista com pesos e viéses.")
} else if (!is.numeric(target)){
stop("Target deve ser um vetor numérico.")
}
# aloca memoria para o tamanho dos dados
yhat <- double(nrow(dados))
# aloca memoria para os valores pre ativação dos 'a'
a <- matrix(nrow = nrow(dados), ncol = 2, dimnames = list(NULL, c("a1", "a2")))
# transpoe os dados para termos acesso aos vetores de x que serao passados para a primeira camada da rede
tdados <- t(as.matrix(dados))
for(i in 1:ncol(tdados)){
yhat[i] <- ffwd(tdados[,i], theta)$yhat
a[i,] <- ffwd(tdados[,i], theta)$pre_activation
}
return(
list(
loss = mean((target - yhat)^2), #média ja entrega soma/m
yhat = yhat,
pre_activation = a,
hidden = matrix(c(phi(a[,1]), phi(a[,2])), ncol = 2)
)
)
}
#| label: setup
#| echo: false
pacman::p_load(tidyverse, NeuralNetTools,latex2exp)
#| label: figf-arquitetura-rede
#| fig-cap: "Arquitetura da rede neural artificial. Adotamos função de ativação sigmoide e linear nas camadas escondidas e de saída, respectivamente."
#| fig-width: 4
#| echo: false
### Figura 2: Arquitetura da RNA.
par(mar=c(0, 0, 0, 0))
wts_in <- rep(1, 9)
struct <- c(2, 2, 1) # dois inputs, dois neurônios escondidos e um otput
plotnet(wts_in, struct = struct,
x_names="", y_names="",
node_labs=F, rel_rsc=.7)
aux <- list(
x=c(-.8, -.8, 0, 0, .8, rep(-.55, 4), -.12, -0.06, .38, .38, .7),
y=c(.73, .28, .73, .28, .5, .78, .68, .48, .32, .88, .5, .68, .44, .7),
rotulo=c("x_1", "x_2", "h_1", "h_2", "\\hat{y}", paste0("w_", 1:4),
"b_1", "b_2", "w_5", "w_6", "b_3")
)
walk(transpose(aux), ~ text(.$x, .$y,
TeX(str_c("$", .$rotulo, "$")), cex=.8))
#| label: feed-forward
# Função de ativação
phi <- function(x) {1/(1 + exp(-x))}
# Função de previsão
ffwd <- function(x, theta) {
if(any(dim(theta[[1]]) != c(2, 2))){
stop("O primeiro elemento de theta deve ser uma matriz de pesos 2x2 para a aplicação nos dados")
} else if(length(theta[[2]]) != 2){
stop("O segundo elemento de theta deve ser um vetor viés de tamanho 2 para somar aos dados")
} else if(any(dim(theta[[3]]) != c(2, 1))){
stop("O terceiro elemento de theta deve ser uma matriz de pesos 2x1 para aplicação na única camada h")
} else if(length(theta[[4]]) != 1){
stop("O quarto elemento de theta deve ser um vetor viés de tamanho 1 para soma na única camada h")
} else if(!is.vector(x) & length(x) != 2){
stop("O x deve ser um vetor de tamanho 2")
}
x <- as.vector(x)
W1 <- theta[[1]]
b1 <- theta[[2]]
W2 <- theta[[3]]
b2 <- theta[[4]]
a <- (t(W1)%*%x)+b1
h <- phi(a)
yhat <- (t(W2)%*%h)+b2
return( # separacao dos elementos de saída para usar no backpropagation
list(yhat = as.double(yhat),
hidden = h,
pre_activation = a)
)
}
#| label: feed-forward-aplicacao
#| depends: feed-forward
#| output: false
x <- c(1,1)
theta <- list(
matrix(c(0.1), nrow = 2, ncol = 2), #w1
c(0.1, 0.1), #b1
matrix(c(0.1), nrow = 2, ncol = 1), #w2
c(0.1) #b2
)
ffwd(x, theta)
#| include: false
rm(x)
#| label: geracao-dados
### Gerando dados "observados"
set.seed(1.2024)
m.obs <- 100000
dados <- tibble(x1.obs=runif(m.obs, -3, 3),
x2.obs=runif(m.obs, -3, 3)) %>%
mutate(mu=abs(x1.obs^3 - 30*sin(x2.obs) + 10),
y=rnorm(m.obs, mean=mu, sd=1))
#| label: loss-function
J.Loss <- function(dados, theta, target){
if(!is.data.frame(dados) | !is_tibble(dados)){
stop("Dados deve ser uma dataframe ou tibble.")
} else if (dim(dados)[2] != 2){
stop("Matriz de dados deve ter 2 colunas.")
} else if (!is.list(theta)){
stop("Theta deve ser uma lista com pesos e viéses.")
} else if (!is.numeric(target)){
stop("Target deve ser um vetor numérico.")
}
# aloca memoria para o tamanho dos dados
yhat <- double(nrow(dados))
# aloca memoria para os valores pre ativação dos 'a'
a <- matrix(nrow = nrow(dados), ncol = 2, dimnames = list(NULL, c("a1", "a2")))
# transpoe os dados para termos acesso aos vetores de x que serao passados para a primeira camada da rede
tdados <- t(as.matrix(dados))
for(i in 1:ncol(tdados)){
yhat[i] <- ffwd(tdados[,i], theta)$yhat
a[i,] <- ffwd(tdados[,i], theta)$pre_activation
}
return(
list(
loss = mean((target - yhat)^2), #média ja entrega soma/m
yhat = yhat,
pre_activation = a,
hidden = matrix(c(phi(a[,1]), phi(a[,2])), ncol = 2)
)
)
}
#| label: separacao-dados
#| depends: geracao-dados
dados_train <- dados[1:80000,]
dados_valid <- dados[80001:90000,]
dados_test <- dados[90001:nrow(dados),]
#| include: false
rm(dados)
#| label: custo-rede
#| depends: feed-forward, loss-function, separacao-dados
#| output: false
# transformamos os dados em matriz
x_test <- dados_test %>%
select(x1.obs, x2.obs)
# separamos o target
y_test <- dados_test %>%
pull(y)
# theta já foi gerado e será reaproveitado
J.Loss(x_test, theta, y_test)$loss
dados <- dados_test[, c(1, 2)]
head(dados)
head(target)
target <- dados_test$y
head(target)
loss_results <- J.Loss(dados, theta, target)
# vetor de yhat
yhat <- loss_results$yhat
# pre_activation da J.Loss retorna o vetor de 'a'
a <- loss_results$pre_activation
head(a)
head(phi(a))
a[,1] <- exp(a[,1])/(1+ exp(a[,1]))^2
a[,2] <- exp(a[,2])/(1+ exp(a[,2]))^2
head(a)
# pre_activation da J.Loss retorna o vetor de 'a'
a <- loss_results$pre_activation
head(exp(a)/(1+exp(a))^2)
a <- loss_results$pre_activation
#transformacao em nos neuronios pre-ativacao
a[,1] <- exp(a[,1])/(1+ exp(a[,1]))^2
a[,2] <- exp(a[,2])/(1+ exp(a[,2]))^2
head(a)
gradiente <- function(dados, theta, target){
if(!is.list(theta)){
stop("Theta deve ser uma lista com pesos e viéses.")
} else if(!is.data.frame(dados) | !is_tibble(dados)){
stop("Dados deve ser uma dataframe ou tibble.")
} else if(!is.numeric(target)){
stop("Target deve ser um vetor numérico.")
}
loss_results <- J.Loss(dados, theta, target)
# vetor de yhat
yhat <- loss_results$yhat
# pre_activation da J.Loss retorna o vetor de 'a'
a <- loss_results$pre_activation
#transformacao em nos neuronios pre-ativacao
a<- exp(a)/(1+ exp(a))^2
# diferenca entre (target) y e yhat
diff <- (target-yhat)
# dados como uma matriz
dados <- as.matrix(dados)
g_w1 <- 2*mean(diff*theta[[3]][1]*dados[,1]*a[,1]) #x1, a1, w5
g_w2 <- 2*mean(diff*theta[[3]][2]*dados[,1]*a[,2]) #x1, a2, w6
g_w3 <- 2*mean(diff*theta[[3]][1]*dados[,2]*a[,1]) #x2, a1, w5
g_w4 <- 2*mean(diff*theta[[3]][2]*dados[,2]*a[,2]) #x2, a2, w6
g_w5 <- 2*mean(diff*(-loss_results$hidden[,1])) #h1
g_w6 <- 2*mean(diff*(-loss_results$hidden[,2])) #h2
g_b1 <- 2*mean(diff*theta[[3]][1]*a[,1]) #a1, w5
g_b2 <- 2*mean(diff*theta[[3]][2]*a[,2]) #a2, w6
g_b3 <- 2*mean(diff*(-1))
grad <- c(g_w1, g_w2, g_w3, g_w4, g_w5, g_w6, g_b1, g_b2, g_b3)
names(grad) <- c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3")
return(grad)
}
gradiente(theta, dados_train[,c(1,2)], dados_train$y) %>%
kable()
gradiente(theta, dados_train[,c(1,2)], dados_train$y) %>%
knitr::kable()
gradiente(theta, dados_train[,c(1,2)], dados_train$y) %>%
as_tibble()
gradiente(theta, dados_train[,c(1,2)], dados_train$y)
#| label: setup
#| echo: false
pacman::p_load(tidyverse, NeuralNetTools,latex2exp)
#| label: figf-arquitetura-rede
#| fig-cap: "Arquitetura da rede neural artificial. Adotamos função de ativação sigmoide e linear nas camadas escondidas e de saída, respectivamente."
#| fig-width: 4
#| echo: false
### Figura 2: Arquitetura da RNA.
par(mar=c(0, 0, 0, 0))
wts_in <- rep(1, 9)
struct <- c(2, 2, 1) # dois inputs, dois neurônios escondidos e um otput
plotnet(wts_in, struct = struct,
x_names="", y_names="",
node_labs=F, rel_rsc=.7)
aux <- list(
x=c(-.8, -.8, 0, 0, .8, rep(-.55, 4), -.12, -0.06, .38, .38, .7),
y=c(.73, .28, .73, .28, .5, .78, .68, .48, .32, .88, .5, .68, .44, .7),
rotulo=c("x_1", "x_2", "h_1", "h_2", "\\hat{y}", paste0("w_", 1:4),
"b_1", "b_2", "w_5", "w_6", "b_3")
)
walk(transpose(aux), ~ text(.$x, .$y,
TeX(str_c("$", .$rotulo, "$")), cex=.8))
#| label: feed-forward
# Função de ativação
phi <- function(x) {1/(1 + exp(-x))}
# Função de previsão
ffwd <- function(x, theta) {
if(any(dim(theta[[1]]) != c(2, 2))){
stop("O primeiro elemento de theta deve ser uma matriz de pesos 2x2 para a aplicação nos dados")
} else if(length(theta[[2]]) != 2){
stop("O segundo elemento de theta deve ser um vetor viés de tamanho 2 para somar aos dados")
} else if(any(dim(theta[[3]]) != c(2, 1))){
stop("O terceiro elemento de theta deve ser uma matriz de pesos 2x1 para aplicação na única camada h")
} else if(length(theta[[4]]) != 1){
stop("O quarto elemento de theta deve ser um vetor viés de tamanho 1 para soma na única camada h")
} else if(!is.vector(x) & length(x) != 2){
stop("O x deve ser um vetor de tamanho 2")
}
x <- as.vector(x)
W1 <- theta[[1]]
b1 <- theta[[2]]
W2 <- theta[[3]]
b2 <- theta[[4]]
a <- (t(W1)%*%x)+b1
h <- phi(a)
yhat <- (t(W2)%*%h)+b2
return( # separacao dos elementos de saída para usar no backpropagation
list(yhat = as.double(yhat),
hidden = h,
pre_activation = a)
)
}
#| label: feed-forward-aplicacao
#| depends: feed-forward
#| output: false
x <- c(1,1)
theta <- list(
matrix(c(0.1), nrow = 2, ncol = 2), #w1
c(0.1, 0.1), #b1
matrix(c(0.1), nrow = 2, ncol = 1), #w2
c(0.1) #b2
)
ffwd(x, theta)
#| include: false
rm(x)
#| label: geracao-dados
### Gerando dados "observados"
set.seed(1.2024)
m.obs <- 100000
dados <- tibble(x1.obs=runif(m.obs, -3, 3),
x2.obs=runif(m.obs, -3, 3)) %>%
mutate(mu=abs(x1.obs^3 - 30*sin(x2.obs) + 10),
y=rnorm(m.obs, mean=mu, sd=1))
#| label: loss-function
J.Loss <- function(dados, theta, target){
if(!is.data.frame(dados) | !is_tibble(dados)){
stop("Dados deve ser uma dataframe ou tibble.")
} else if (dim(dados)[2] != 2){
stop("Matriz de dados deve ter 2 colunas.")
} else if (!is.list(theta)){
stop("Theta deve ser uma lista com pesos e viéses.")
} else if (!is.numeric(target)){
stop("Target deve ser um vetor numérico.")
}
# aloca memoria para o tamanho dos dados
yhat <- double(nrow(dados))
# aloca memoria para os valores pre ativação dos 'a'
a <- matrix(nrow = nrow(dados), ncol = 2, dimnames = list(NULL, c("a1", "a2")))
# transpoe os dados para termos acesso aos vetores de x que serao passados para a primeira camada da rede
tdados <- t(as.matrix(dados))
for(i in 1:ncol(tdados)){
yhat[i] <- ffwd(tdados[,i], theta)$yhat
a[i,] <- ffwd(tdados[,i], theta)$pre_activation
}
return(
list(
loss = mean((target - yhat)^2), #média ja entrega soma/m
yhat = yhat,
pre_activation = a,
hidden = matrix(c(phi(a[,1]), phi(a[,2])), ncol = 2)
)
)
}
#| label: separacao-dados
#| depends: geracao-dados
dados_train <- dados[1:80000,]
dados_valid <- dados[80001:90000,]
dados_test <- dados[90001:nrow(dados),]
#| include: false
rm(dados)
#| label: custo-rede
#| depends: feed-forward, loss-function, separacao-dados
#| output: false
# transformamos os dados em matriz
x_test <- dados_test %>%
select(x1.obs, x2.obs)
# separamos o target
y_test <- dados_test %>%
pull(y)
# theta já foi gerado e será reaproveitado
J.Loss(x_test, theta, y_test)$loss
#| label: gradiente
gradiente <- function(dados, theta, target){
if(!is.list(theta)){
stop("Theta deve ser uma lista com pesos e viéses.")
} else if(!is.data.frame(dados) | !is_tibble(dados)){
stop("Dados deve ser uma dataframe ou tibble.")
} else if(!is.numeric(target)){
stop("Target deve ser um vetor numérico.")
}
loss_results <- J.Loss(dados, theta, target)
# vetor de yhat
yhat <- loss_results$yhat
# pre_activation da J.Loss retorna o vetor de 'a'
a <- loss_results$pre_activation
#transformacao em nos neuronios pre-ativacao
a<- exp(a)/(1+ exp(a))^2
# diferenca entre (target) y e yhat
diff <- (target-yhat)
# dados como uma matriz
dados <- as.matrix(dados)
g_w1 <- 2*mean(diff*theta[[3]][1]*dados[,1]*a[,1]) #x1, a1, w5
g_w2 <- 2*mean(diff*theta[[3]][2]*dados[,1]*a[,2]) #x1, a2, w6
g_w3 <- 2*mean(diff*theta[[3]][1]*dados[,2]*a[,1]) #x2, a1, w5
g_w4 <- 2*mean(diff*theta[[3]][2]*dados[,2]*a[,2]) #x2, a2, w6
g_w5 <- 2*mean(diff*(-loss_results$hidden[,1])) #h1
g_w6 <- 2*mean(diff*(-loss_results$hidden[,2])) #h2
g_b1 <- 2*mean(diff*theta[[3]][1]*a[,1]) #a1, w5
g_b2 <- 2*mean(diff*theta[[3]][2]*a[,2]) #a2, w6
g_b3 <- 2*mean(diff*(-1))
grad <- c(g_w1, g_w2, g_w3, g_w4, g_w5, g_w6, g_b1, g_b2, g_b3)
names(grad) <- c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3")
return(grad)
}
gradiente(theta, dados_train[,c(1,2)], dados_train$y)
dadtheta
theta
head(dados_train[,c(1,2)])
head(dados_train$y)
gradiente(dados_train[,c(1,2)], theta, dados_train$y) %>%
as_tibble()
gradiente(dados_train[,c(1,2)], theta, dados_train$y) %>%
as.data.frame()
gradiente(dados_train[,c(1,2)], theta, dados_train$y) %>%
as.data.frame() %>% dim
gradiente(dados_train[,c(1,2)], theta, dados_train$y) %>%
as_tibble() %>%
mutate(Componente = c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3"),
Derivada = .)
gradiente(dados_train[,c(1,2)], theta, dados_train$y) %>%
as_tibble() %>%
mutate(Componente = c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3"),
Derivada = .$value)
gradiente(dados_train[,c(1,2)], theta, dados_train$y) %>%
as_tibble() %>%
summarise(Componente = c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3"),
Derivada = .$value)
gradiente(dados_train[,c(1,2)], theta, dados_train$y) %>%
as_tibble() %>%
summarise(Componente = c("w1", "w2", "w3", "w4", "w5", "w6", "b1", "b2", "b3"),
Derivada = .$value) %>%
knitr::kable()
